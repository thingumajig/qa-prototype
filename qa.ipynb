{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "qa.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thingumajig/qa-prototype/blob/master/qa.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vC1s1yF99t5M",
        "colab_type": "text"
      },
      "source": [
        "# Init"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgUEga5G9y_w",
        "colab_type": "text"
      },
      "source": [
        "## Install packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IiEQ3rLjNRzG",
        "colab_type": "code",
        "outputId": "d9759e5f-5268-43d5-e11d-d25a33d8fe77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        }
      },
      "source": [
        "!pip install spacy\n",
        "\n",
        "!pip3 uninstall --quiet --yes tensorflow\n",
        "!pip3 install --quiet tensorflow-gpu==1.14.0\n",
        "!pip3 install --quiet tensorflow-hub\n",
        "!pip3 install --quiet sentencepiece==0.1.83\n",
        "!pip3 install --quiet tf-sentencepiece==0.1.83\n",
        "!pip3 install --quiet simpleneighbors\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (2.1.8)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.0.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.1.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: blis<0.3.0,>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.2.4)\n",
            "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.9.6)\n",
            "Requirement already satisfied: thinc<7.1.0,>=7.0.8 in /usr/local/lib/python3.6/dist-packages (from spacy) (7.0.8)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.21.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.3.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.17.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<7.1.0,>=7.0.8->spacy) (4.28.1)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2019.9.11)\n",
            "\u001b[K     |████████████████████████████████| 377.0MB 46kB/s \n",
            "\u001b[K     |████████████████████████████████| 3.2MB 32.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 491kB 39.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.0MB 2.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.7MB 2.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 645kB 3.9MB/s \n",
            "\u001b[?25h  Building wheel for annoy (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5oCPLvd-AQZ",
        "colab_type": "text"
      },
      "source": [
        "## Initialize grammar parser"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4weL6rPINbY5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "# doc = nlp(\"With respect to all losses caused by the peril of Flood, the Company shall not be liable, in the aggregate for any one Policy year, for more than its proportionate share of US$25,000,000.\")\n",
        "# doc = nlp(\"The Program limit of liability is US$400,000,000.\")\n",
        "# print(doc)\n",
        "# for token in doc:\n",
        "#     print(\"{2}-{1}({3}-{6}, {0}-{5})\".format(token.text, token.tag_, token.dep_, token.head.text, token.head.tag_, token.i+1, token.head.i+1))\n",
        "# for np in doc.noun_chunks:\n",
        "#   print(np.text)\n",
        "\n",
        "\n",
        "from spacy.symbols import *\n",
        "\n",
        "np_labels = set([nsubj, nsubjpass, dobj, iobj, pobj]) # Probably others too\n",
        "np_labels_full = set([nsubj, nsubjpass, dobj, iobj, pobj, csubj, csubjpass, attr]) # Probably others too\n",
        "# print(dir(spacy.symbols))\n",
        "# subj - subject\n",
        "# nsubj - nominal subject\n",
        "# nsubjpass - passive nominal subject\n",
        "# csubj - clausal subject\n",
        "# csubjpass - passive clausal subject\n",
        "\n",
        "def iter_nps(doc):\n",
        "    for word in doc:\n",
        "        if word.dep in np_labels_full:\n",
        "            yield word\n",
        "\n",
        "def iter_nps_str(doc):\n",
        "  s = ''\n",
        "  for np in iter_nps(doc):\n",
        "    for t in np.subtree:\n",
        "      s += str(t)+' '\n",
        "    yield s.strip()\n",
        "    s = ''\n",
        "\n",
        "\n",
        "# print('='*20)\n",
        "# for np in iter_nps(doc):\n",
        "#    print(np)\n",
        "#    for t in np.subtree:\n",
        "#      print(f'\\t{t}')\n",
        "\n",
        "# print('='*20)\n",
        "# for np in iter_nps_str(doc):\n",
        "#   print(np)\n",
        "\n",
        "\n",
        "# print('='*20)\n",
        "# for token in doc:\n",
        "#   print(f'{token.text} {token.tag_} {token.dep_} {str(token.dep)} \\t\\t\\thead: {token.head.tag_} {token.head.dep_} {token.head.text}')     \n",
        "#   for t in token.subtree:\n",
        "#     print(f'\\t{t}')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqmuFxy0Ylpa",
        "colab_type": "text"
      },
      "source": [
        "##  Set up Tensorflow graph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AV9S-wKeWG8A",
        "colab_type": "code",
        "outputId": "56dc638a-92de-4c22-f9e2-3fa96758d026",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 604
        }
      },
      "source": [
        "%%time\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import numpy as np\n",
        "import tf_sentencepiece\n",
        "\n",
        "# Set up graph.\n",
        "g = tf.Graph()\n",
        "with g.as_default():\n",
        "  questions_input = tf.placeholder(dtype=tf.string, shape=[None])\n",
        "  responses_input = tf.placeholder(dtype=tf.string, shape=[None])\n",
        "  contexts_input = tf.placeholder(dtype=tf.string, shape=[None])\n",
        "\n",
        "\n",
        "  module = hub.Module(\"https://tfhub.dev/google/universal-sentence-encoder-multilingual-qa/1\")\n",
        "  question_embeddings = module(\n",
        "    dict(input=questions_input),\n",
        "    signature=\"question_encoder\", as_dict=True)\n",
        "\n",
        "  response_embeddings = module(\n",
        "    dict(input=responses_input,\n",
        "         context=contexts_input),\n",
        "    signature=\"response_encoder\", as_dict=True)\n",
        "\n",
        "  init_op = tf.group([tf.global_variables_initializer(), tf.tables_initializer()])\n",
        "g.finalize()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 35.9 s, sys: 1.89 s, total: 37.8 s\n",
            "Wall time: 38.8 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdHhNc-_-1Jn",
        "colab_type": "text"
      },
      "source": [
        "# Run"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXQYvsUn-6g2",
        "colab_type": "text"
      },
      "source": [
        "##Initialize tensorflow session."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zqSZ5-4KtDTp",
        "colab_type": "code",
        "outputId": "271a857d-cbec-442c-8919-1036588f612a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "%%time\n",
        "# Initialize session.\n",
        "session = tf.Session(graph=g)\n",
        "session.run(init_op)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 10.5 s, sys: 2 s, total: 12.5 s\n",
            "Wall time: 14.3 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWWWJ_LPC9Hw",
        "colab_type": "text"
      },
      "source": [
        "## Tests"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pu_3kKA8ZpXC",
        "colab_type": "code",
        "outputId": "60032be6-570f-41ab-a82a-b83d7a918e4e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# %%time\n",
        "sentences = '''\n",
        "F.\tPROGRAM LIMITS OF LIABILITY\n",
        "\n",
        "(1)\tThe Program limit of liability is US$400,000,000. \n",
        "\n",
        "(2)\tSublimits below are applicable to all direct physical loss, damage or destruction insured against, except an Accident. \n",
        "\n",
        "(a)\tWith respect to all losses caused by the peril of Flood, the Company shall not be liable, in the aggregate for any one Policy year, for more than its proportionate share of US$25,000,000. \n",
        "\n",
        "But not to exceed US$5,000,000 in the aggregate for any one Policy year\n",
        "for the peril of Flood occurring in Special Flood Hazard Areas. However,\n",
        "this Special Flood Hazard Area sublimit shall not apply to loss involving \n",
        "personal property, buildings, or structures wholly located outside an area\n",
        "designated as a Special Flood Hazard Area.\n",
        "\n",
        "The aggregate limit stated in the paragraph above shall be part of the overall aggregate limit stated in the introduction to this paragraph.\n",
        "\n",
        "Even if the peril of Flood is the predominant cause of direct physical loss, damage or destruction, any ensuing physical loss, damage or destruction arising from a peril not otherwise excluded herein shall not be subject to the sublimit or aggregate specified in this paragraph (a).\n",
        "'''\n",
        "sentence1 = 'With respect to all losses caused by the peril of Earthquake, the Company shall not be liable, in the aggregate for any one Policy year, for more than its proportionate share of US$50,000,000.'\n",
        "\n",
        "sentence2 = '''\n",
        "Суд освободил из-под стражи следователя Алексея Шиманского и полицейского Кирилла Лисового из Санкт-Петербурга, которых подозревают в посредничестве при передаче взятки в 19 миллионов рублей. Об этом сообщает «Фонтанка».\n",
        "'''\n",
        "\n",
        "import simpleneighbors\n",
        "\n",
        "def  qa(sentences, queries, use_parser = True):\n",
        "  index = simpleneighbors.SimpleNeighbors(\n",
        "      512, metric='angular')\n",
        "\n",
        "  def candidate_generation(t, from_n=1, to_n=6):\n",
        "    l = t.split()\n",
        "    grams = []\n",
        "    for n in range(from_n, to_n):\n",
        "      grams += [' '.join(l[i:i+n]) for i in range(len(l)-n+1)]\n",
        "    \n",
        "    return grams\n",
        "\n",
        "\n",
        "  def display_nearest_neighbors(query_text):\n",
        "    print(f'Query: {query_text}')\n",
        "    query_embedding = session.run(question_embeddings, feed_dict={questions_input: [query_text]})['outputs'][0]\n",
        "    # print(\"query_embedding:\")\n",
        "    # print(query_embedding)\n",
        "    search_results = index.nearest(query_embedding, n=10)\n",
        "    print('Top-10 Responses:')\n",
        "    for s in search_results:\n",
        "      print(f'\\t{s}')\n",
        "\n",
        "\n",
        "  def calculate_sentences_emb(sentences, use_parser=True):\n",
        "    responses = []\n",
        "    contexts = []\n",
        "    if use_parser:\n",
        "      doc = nlp(sentences)\n",
        "      for sent in doc.sents:\n",
        "        # print('='*40)\n",
        "        # print(sent)\n",
        "        # print('-- Noun groups & attributes: --')\n",
        "        for np in iter_nps_str(sent):\n",
        "          # print(f'\\t{np}')\n",
        "          responses.append(str(np).strip())\n",
        "          contexts.append(str(sent).strip())\n",
        "    else:\n",
        "      grams = candidate_generation(sentences, 1, 5)\n",
        "      for g in grams:\n",
        "        responses.append(g.strip())\n",
        "        contexts.append(sentences.strip())\n",
        "\n",
        "\n",
        "    candidate_embeddings = session.run(\n",
        "        response_embeddings,\n",
        "        feed_dict={\n",
        "            responses_input: responses,\n",
        "            contexts_input: contexts\n",
        "        })\n",
        "    print(f'Shape: {candidate_embeddings[\"outputs\"].shape}')\n",
        "    # print(candidate_embeddings[\"outputs\"][0])\n",
        "    print('Candidates:')\n",
        "    for i in range(len(responses)):\n",
        "      print(f'\\t{responses[i]}')\n",
        "      index.add_one(responses[i], candidate_embeddings['outputs'][i])\n",
        "\n",
        "    index.build()\n",
        "    \n",
        "  calculate_sentences_emb(sentences, use_parser)\n",
        "  # display_nearest_neighbors('Какие имена у освобождённых')\n",
        "  if isinstance(queries, list):\n",
        "    for q in queries:\n",
        "      display_nearest_neighbors(q)\n",
        "  else:\n",
        "      display_nearest_neighbors(queries)\n",
        "\n",
        "\n",
        "\n",
        "# qa(sentences, 'имена отпущенных')\n",
        "qa(sentence1, ['peril','sum'])\n",
        "qa(sentence2, ['имена отпущенных', 'сумма взятки'])\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape: (10, 512)\n",
            "Candidates:\n",
            "\trespect to all losses caused by the peril of Earthquake\n",
            "\tall losses caused by the peril of Earthquake\n",
            "\tthe peril of Earthquake\n",
            "\tEarthquake\n",
            "\tthe Company\n",
            "\tthe aggregate for any one Policy year\n",
            "\tany one Policy year\n",
            "\tmore than its proportionate share of US$ 50,000,000\n",
            "\tits proportionate share of US$ 50,000,000\n",
            "\tUS$ 50,000,000\n",
            "Query: peril\n",
            "Top-10 Responses:\n",
            "\tthe peril of Earthquake\n",
            "\trespect to all losses caused by the peril of Earthquake\n",
            "\tall losses caused by the peril of Earthquake\n",
            "\tthe Company\n",
            "\tEarthquake\n",
            "\tany one Policy year\n",
            "\tUS$ 50,000,000\n",
            "\tmore than its proportionate share of US$ 50,000,000\n",
            "\tthe aggregate for any one Policy year\n",
            "\tits proportionate share of US$ 50,000,000\n",
            "Query: sum\n",
            "Top-10 Responses:\n",
            "\tmore than its proportionate share of US$ 50,000,000\n",
            "\tits proportionate share of US$ 50,000,000\n",
            "\tthe aggregate for any one Policy year\n",
            "\tUS$ 50,000,000\n",
            "\tEarthquake\n",
            "\tthe Company\n",
            "\trespect to all losses caused by the peril of Earthquake\n",
            "\tthe peril of Earthquake\n",
            "\tall losses caused by the peril of Earthquake\n",
            "\tany one Policy year\n",
            "Shape: (8, 512)\n",
            "Candidates:\n",
            "\tСуд\n",
            "\tиз - под стражи\n",
            "\tАлексея Шиманского\n",
            "\tКирилла Лисового из Санкт - Петербурга ,\n",
            "\tкоторых\n",
            "\tпосредничестве\n",
            "\tпри передаче\n",
            "\tв 19 миллионов рублей\n",
            "Query: имена отпущенных\n",
            "Top-10 Responses:\n",
            "\tАлексея Шиманского\n",
            "\tКирилла Лисового из Санкт - Петербурга ,\n",
            "\tкоторых\n",
            "\tСуд\n",
            "\tпри передаче\n",
            "\tпосредничестве\n",
            "\tиз - под стражи\n",
            "\tв 19 миллионов рублей\n",
            "Query: сумма взятки\n",
            "Top-10 Responses:\n",
            "\tв 19 миллионов рублей\n",
            "\tСуд\n",
            "\tкоторых\n",
            "\tпосредничестве\n",
            "\tпри передаче\n",
            "\tиз - под стражи\n",
            "\tАлексея Шиманского\n",
            "\tКирилла Лисового из Санкт - Петербурга ,\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctGvm-GsFPW7",
        "colab_type": "text"
      },
      "source": [
        "# Form"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4V2drB5rE1TJ",
        "colab_type": "code",
        "cellView": "both",
        "outputId": "f276bb90-de1d-412e-cc1d-4c97309c0c87",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "#@title Question and answer\n",
        "text = \"\\u0410\\u0441\\u0442\\u0440\\u043E\\u043D\\u043E\\u043C\\u044B \\u0418\\u043D\\u0441\\u0442\\u0438\\u0442\\u0443\\u0442\\u0430 \\u0432\\u043D\\u0435\\u0437\\u0435\\u043C\\u043D\\u043E\\u0439 \\u0444\\u0438\\u0437\\u0438\\u043A\\u0438 \\u041E\\u0431\\u0449\\u0435\\u0441\\u0442\\u0432\\u0430 \\u041C\\u0430\\u043A\\u0441\\u0430 \\u041F\\u043B\\u0430\\u043D\\u043A\\u0430 \\u0432 \\u0413\\u0435\\u0440\\u043C\\u0430\\u043D\\u0438\\u0438 \\u0441\\u043E\\u043E\\u0431\\u0449\\u0438\\u043B\\u0438 \\u043E \\u0432\\u0441\\u043F\\u043B\\u0435\\u0441\\u043A\\u0435 \\u0430\\u043A\\u0442\\u0438\\u0432\\u043D\\u043E\\u0441\\u0442\\u0438 \\u043D\\u0435\\u0438\\u0437\\u0432\\u0435\\u0441\\u0442\\u043D\\u043E\\u0433\\u043E \\u0438\\u0441\\u0442\\u043E\\u0447\\u043D\\u0438\\u043A\\u0430 \\u0440\\u0435\\u043D\\u0442\\u0433\\u0435\\u043D\\u043E\\u0432\\u0441\\u043A\\u0438\\u0445 \\u043B\\u0443\\u0447\\u0435\\u0439 \\u0432 \\u0433\\u0430\\u043B\\u0430\\u043A\\u0442\\u0438\\u043A\\u0435 NGC 300, \\u0440\\u0430\\u0441\\u043F\\u043E\\u043B\\u043E\\u0436\\u0435\\u043D\\u043D\\u043E\\u0439 \\u0432 \\u0441\\u0435\\u043C\\u0438 \\u043C\\u0438\\u043B\\u043B\\u0438\\u043E\\u043D\\u0430\\u0445 \\u0441\\u0432\\u0435\\u0442\\u043E\\u0432\\u044B\\u0445 \\u043B\\u0435\\u0442 \\u043E\\u0442 \\u0417\\u0435\\u043C\\u043B\\u0438.\" #@param {type:\"string\"}\n",
        "query = \"\\u043D\\u0430 \\u043A\\u0430\\u043A\\u043E\\u043C \\u0440\\u0430\\u0441\\u0441\\u0442\\u043E\\u044F\\u043D\\u0438\\u0438?\" #@param {type:\"string\"}\n",
        "use_token_ngram = True #@param {type:\"boolean\"}\n",
        "\n",
        "qa(text, [query], not use_token_ngram)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape: (110, 512)\n",
            "Candidates:\n",
            "\tАстрономы\n",
            "\tИнститута\n",
            "\tвнеземной\n",
            "\tфизики\n",
            "\tОбщества\n",
            "\tМакса\n",
            "\tПланка\n",
            "\tв\n",
            "\tГермании\n",
            "\tсообщили\n",
            "\tо\n",
            "\tвсплеске\n",
            "\tактивности\n",
            "\tнеизвестного\n",
            "\tисточника\n",
            "\tрентгеновских\n",
            "\tлучей\n",
            "\tв\n",
            "\tгалактике\n",
            "\tNGC\n",
            "\t300,\n",
            "\tрасположенной\n",
            "\tв\n",
            "\tсеми\n",
            "\tмиллионах\n",
            "\tсветовых\n",
            "\tлет\n",
            "\tот\n",
            "\tЗемли.\n",
            "\tАстрономы Института\n",
            "\tИнститута внеземной\n",
            "\tвнеземной физики\n",
            "\tфизики Общества\n",
            "\tОбщества Макса\n",
            "\tМакса Планка\n",
            "\tПланка в\n",
            "\tв Германии\n",
            "\tГермании сообщили\n",
            "\tсообщили о\n",
            "\tо всплеске\n",
            "\tвсплеске активности\n",
            "\tактивности неизвестного\n",
            "\tнеизвестного источника\n",
            "\tисточника рентгеновских\n",
            "\tрентгеновских лучей\n",
            "\tлучей в\n",
            "\tв галактике\n",
            "\tгалактике NGC\n",
            "\tNGC 300,\n",
            "\t300, расположенной\n",
            "\tрасположенной в\n",
            "\tв семи\n",
            "\tсеми миллионах\n",
            "\tмиллионах световых\n",
            "\tсветовых лет\n",
            "\tлет от\n",
            "\tот Земли.\n",
            "\tАстрономы Института внеземной\n",
            "\tИнститута внеземной физики\n",
            "\tвнеземной физики Общества\n",
            "\tфизики Общества Макса\n",
            "\tОбщества Макса Планка\n",
            "\tМакса Планка в\n",
            "\tПланка в Германии\n",
            "\tв Германии сообщили\n",
            "\tГермании сообщили о\n",
            "\tсообщили о всплеске\n",
            "\tо всплеске активности\n",
            "\tвсплеске активности неизвестного\n",
            "\tактивности неизвестного источника\n",
            "\tнеизвестного источника рентгеновских\n",
            "\tисточника рентгеновских лучей\n",
            "\tрентгеновских лучей в\n",
            "\tлучей в галактике\n",
            "\tв галактике NGC\n",
            "\tгалактике NGC 300,\n",
            "\tNGC 300, расположенной\n",
            "\t300, расположенной в\n",
            "\tрасположенной в семи\n",
            "\tв семи миллионах\n",
            "\tсеми миллионах световых\n",
            "\tмиллионах световых лет\n",
            "\tсветовых лет от\n",
            "\tлет от Земли.\n",
            "\tАстрономы Института внеземной физики\n",
            "\tИнститута внеземной физики Общества\n",
            "\tвнеземной физики Общества Макса\n",
            "\tфизики Общества Макса Планка\n",
            "\tОбщества Макса Планка в\n",
            "\tМакса Планка в Германии\n",
            "\tПланка в Германии сообщили\n",
            "\tв Германии сообщили о\n",
            "\tГермании сообщили о всплеске\n",
            "\tсообщили о всплеске активности\n",
            "\tо всплеске активности неизвестного\n",
            "\tвсплеске активности неизвестного источника\n",
            "\tактивности неизвестного источника рентгеновских\n",
            "\tнеизвестного источника рентгеновских лучей\n",
            "\tисточника рентгеновских лучей в\n",
            "\tрентгеновских лучей в галактике\n",
            "\tлучей в галактике NGC\n",
            "\tв галактике NGC 300,\n",
            "\tгалактике NGC 300, расположенной\n",
            "\tNGC 300, расположенной в\n",
            "\t300, расположенной в семи\n",
            "\tрасположенной в семи миллионах\n",
            "\tв семи миллионах световых\n",
            "\tсеми миллионах световых лет\n",
            "\tмиллионах световых лет от\n",
            "\tсветовых лет от Земли.\n",
            "Query: на каком расстоянии?\n",
            "Top-10 Responses:\n",
            "\tвнеземной\n",
            "\tот Земли.\n",
            "\tлучей в\n",
            "\tрасположенной\n",
            "\tрасположенной в семи миллионах\n",
            "\tлет от Земли.\n",
            "\tрасположенной в\n",
            "\tв галактике\n",
            "\tрасположенной в семи\n",
            "\tрентгеновских лучей в\n",
            "CPU times: user 161 ms, sys: 20 ms, total: 181 ms\n",
            "Wall time: 178 ms\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}